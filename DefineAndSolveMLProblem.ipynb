{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8: Define and Solve an ML Problem of Your Choosing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab assignment, you will follow the machine learning life cycle and implement a model to solve a machine learning problem of your choosing. You will select a data set and choose a predictive problem that the data set supports.  You will then inspect the data with your problem in mind and begin to formulate a  project plan. You will then implement the machine learning project plan. \n",
    "\n",
    "You will complete the following tasks:\n",
    "\n",
    "1. Build Your DataFrame\n",
    "2. Define Your ML Problem\n",
    "3. Perform exploratory data analysis to understand your data.\n",
    "4. Define Your Project Plan\n",
    "5. Implement Your Project Plan:\n",
    "    * Prepare your data for your model.\n",
    "    * Fit your model to the training data and evaluate your model.\n",
    "    * Improve your model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Build Your DataFrame\n",
    "\n",
    "You will have the option to choose one of four data sets that you have worked with in this program:\n",
    "\n",
    "* The \"census\" data set that contains Census information from 1994: `censusData.csv`\n",
    "* Airbnb NYC \"listings\" data set: `airbnbListingsData.csv`\n",
    "* World Happiness Report (WHR) data set: `WHR2018Chapter2OnlineData.csv`\n",
    "* Book Review data set: `bookReviewsData.csv`\n",
    "\n",
    "Note that these are variations of the data sets that you have worked with in this program. For example, some do not include some of the preprocessing necessary for specific models. \n",
    "\n",
    "#### Load a Data Set and Save it as a Pandas DataFrame\n",
    "\n",
    "The code cell below contains filenames (path + filename) for each of the four data sets available to you.\n",
    "\n",
    "<b>Task:</b> In the code cell below, use the same method you have been using to load the data using `pd.read_csv()` and save it to DataFrame `df`. \n",
    "\n",
    "You can load each file as a new DataFrame to inspect the data before choosing your data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>neighborhood_overview</th>\n",
       "      <th>host_name</th>\n",
       "      <th>host_location</th>\n",
       "      <th>host_about</th>\n",
       "      <th>host_response_rate</th>\n",
       "      <th>host_acceptance_rate</th>\n",
       "      <th>host_is_superhost</th>\n",
       "      <th>host_listings_count</th>\n",
       "      <th>...</th>\n",
       "      <th>review_scores_communication</th>\n",
       "      <th>review_scores_location</th>\n",
       "      <th>review_scores_value</th>\n",
       "      <th>instant_bookable</th>\n",
       "      <th>calculated_host_listings_count</th>\n",
       "      <th>calculated_host_listings_count_entire_homes</th>\n",
       "      <th>calculated_host_listings_count_private_rooms</th>\n",
       "      <th>calculated_host_listings_count_shared_rooms</th>\n",
       "      <th>reviews_per_month</th>\n",
       "      <th>n_host_verifications</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Skylit Midtown Castle</td>\n",
       "      <td>Beautiful, spacious skylit studio in the heart...</td>\n",
       "      <td>Centrally located in the heart of Manhattan ju...</td>\n",
       "      <td>Jennifer</td>\n",
       "      <td>New York, New York, United States</td>\n",
       "      <td>A New Yorker since 2000! My passion is creatin...</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.17</td>\n",
       "      <td>True</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.79</td>\n",
       "      <td>4.86</td>\n",
       "      <td>4.41</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.33</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Whole flr w/private bdrm, bath &amp; kitchen(pls r...</td>\n",
       "      <td>Enjoy 500 s.f. top floor in 1899 brownstone, w...</td>\n",
       "      <td>Just the right mix of urban center and local n...</td>\n",
       "      <td>LisaRoxanne</td>\n",
       "      <td>New York, New York, United States</td>\n",
       "      <td>Laid-back Native New Yorker (formerly bi-coast...</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.69</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.80</td>\n",
       "      <td>4.71</td>\n",
       "      <td>4.64</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.86</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spacious Brooklyn Duplex, Patio + Garden</td>\n",
       "      <td>We welcome you to stay in our lovely 2 br dupl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rebecca</td>\n",
       "      <td>Brooklyn, New York, United States</td>\n",
       "      <td>Rebecca is an artist/designer, and Henoch is i...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.50</td>\n",
       "      <td>5.00</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Large Furnished Room Near B'way</td>\n",
       "      <td>Please don’t expect the luxury here just a bas...</td>\n",
       "      <td>Theater district, many restaurants around here.</td>\n",
       "      <td>Shunichi</td>\n",
       "      <td>New York, New York, United States</td>\n",
       "      <td>I used to work for a financial industry but no...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.42</td>\n",
       "      <td>4.87</td>\n",
       "      <td>4.36</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.68</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cozy Clean Guest Room - Family Apt</td>\n",
       "      <td>Our best guests are seeking a safe, clean, spa...</td>\n",
       "      <td>Our neighborhood is full of restaurants and ca...</td>\n",
       "      <td>MaryEllen</td>\n",
       "      <td>New York, New York, United States</td>\n",
       "      <td>Welcome to family life with my oldest two away...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.95</td>\n",
       "      <td>4.94</td>\n",
       "      <td>4.92</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.87</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "0                              Skylit Midtown Castle   \n",
       "1  Whole flr w/private bdrm, bath & kitchen(pls r...   \n",
       "2           Spacious Brooklyn Duplex, Patio + Garden   \n",
       "3                   Large Furnished Room Near B'way　   \n",
       "4                 Cozy Clean Guest Room - Family Apt   \n",
       "\n",
       "                                         description  \\\n",
       "0  Beautiful, spacious skylit studio in the heart...   \n",
       "1  Enjoy 500 s.f. top floor in 1899 brownstone, w...   \n",
       "2  We welcome you to stay in our lovely 2 br dupl...   \n",
       "3  Please don’t expect the luxury here just a bas...   \n",
       "4  Our best guests are seeking a safe, clean, spa...   \n",
       "\n",
       "                               neighborhood_overview    host_name  \\\n",
       "0  Centrally located in the heart of Manhattan ju...     Jennifer   \n",
       "1  Just the right mix of urban center and local n...  LisaRoxanne   \n",
       "2                                                NaN      Rebecca   \n",
       "3    Theater district, many restaurants around here.     Shunichi   \n",
       "4  Our neighborhood is full of restaurants and ca...    MaryEllen   \n",
       "\n",
       "                       host_location  \\\n",
       "0  New York, New York, United States   \n",
       "1  New York, New York, United States   \n",
       "2  Brooklyn, New York, United States   \n",
       "3  New York, New York, United States   \n",
       "4  New York, New York, United States   \n",
       "\n",
       "                                          host_about  host_response_rate  \\\n",
       "0  A New Yorker since 2000! My passion is creatin...                0.80   \n",
       "1  Laid-back Native New Yorker (formerly bi-coast...                0.09   \n",
       "2  Rebecca is an artist/designer, and Henoch is i...                1.00   \n",
       "3  I used to work for a financial industry but no...                1.00   \n",
       "4  Welcome to family life with my oldest two away...                 NaN   \n",
       "\n",
       "   host_acceptance_rate  host_is_superhost  host_listings_count  ...  \\\n",
       "0                  0.17               True                  8.0  ...   \n",
       "1                  0.69               True                  1.0  ...   \n",
       "2                  0.25               True                  1.0  ...   \n",
       "3                  1.00               True                  1.0  ...   \n",
       "4                   NaN               True                  1.0  ...   \n",
       "\n",
       "   review_scores_communication  review_scores_location  review_scores_value  \\\n",
       "0                         4.79                    4.86                 4.41   \n",
       "1                         4.80                    4.71                 4.64   \n",
       "2                         5.00                    4.50                 5.00   \n",
       "3                         4.42                    4.87                 4.36   \n",
       "4                         4.95                    4.94                 4.92   \n",
       "\n",
       "  instant_bookable calculated_host_listings_count  \\\n",
       "0            False                              3   \n",
       "1            False                              1   \n",
       "2            False                              1   \n",
       "3            False                              1   \n",
       "4            False                              1   \n",
       "\n",
       "   calculated_host_listings_count_entire_homes  \\\n",
       "0                                            3   \n",
       "1                                            1   \n",
       "2                                            1   \n",
       "3                                            0   \n",
       "4                                            0   \n",
       "\n",
       "   calculated_host_listings_count_private_rooms  \\\n",
       "0                                             0   \n",
       "1                                             0   \n",
       "2                                             0   \n",
       "3                                             1   \n",
       "4                                             1   \n",
       "\n",
       "   calculated_host_listings_count_shared_rooms  reviews_per_month  \\\n",
       "0                                            0               0.33   \n",
       "1                                            0               4.86   \n",
       "2                                            0               0.02   \n",
       "3                                            0               3.68   \n",
       "4                                            0               0.87   \n",
       "\n",
       "  n_host_verifications  \n",
       "0                    9  \n",
       "1                    6  \n",
       "2                    3  \n",
       "3                    4  \n",
       "4                    7  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# File names of the four data sets\n",
    "adultDataSet_filename = os.path.join(os.getcwd(), \"data\", \"censusData.csv\")\n",
    "airbnbDataSet_filename = os.path.join(os.getcwd(), \"data\", \"airbnbListingsData.csv\")\n",
    "WHRDataSet_filename = os.path.join(os.getcwd(), \"data\", \"WHR2018Chapter2OnlineData.csv\")\n",
    "bookReviewDataSet_filename = os.path.join(os.getcwd(), \"data\", \"bookReviewsData.csv\")\n",
    "\n",
    "\n",
    "df = pd.read_csv(airbnbDataSet_filename)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Define Your ML Problem\n",
    "\n",
    "Next you will formulate your ML Problem. In the markdown cell below, answer the following questions:\n",
    "\n",
    "1. List the data set you have chosen.\n",
    "2. What will you be predicting? What is the label?\n",
    "3. Is this a supervised or unsupervised learning problem? Is this a clustering, classification or regression problem? Is it a binary classificaiton or multi-class classifiction problem?\n",
    "4. What are your features? (note: this list may change after your explore your data)\n",
    "5. Explain why this is an important problem. In other words, how would a company create value with a model that predicts this label?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have chosen the airbnb dataset which will be predicting the price of a certain listing, making the label would be the price coulmn. It will be a supervised learning problem, specifically a regression task, since we are predicting a continuous value. Some important features that I'll include would be room_type, accommodates, bathrooms, bedrooms, beds, minimum_nights, availability_365, number_of_reviews, and review_scores_rating. Predicting rental prices is valuable because it helps the hosts support flexible pricing and provide insights for market analysis and cost management. I would consider this an important problem because accurate predictions can boost host cost revenue and enhance user satisfaction by offering better pricing information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Understand Your Data\n",
    "\n",
    "The next step is to perform exploratory data analysis. Inspect and analyze your data set with your machine learning problem in mind. Consider the following as you inspect your data:\n",
    "\n",
    "1. What data preparation techniques would you like to use? These data preparation techniques may include:\n",
    "\n",
    "    * addressing missingness, such as replacing missing values with means\n",
    "    * finding and replacing outliers\n",
    "    * renaming features and labels\n",
    "    * finding and replacing outliers\n",
    "    * performing feature engineering techniques such as one-hot encoding on categorical features\n",
    "    * selecting appropriate features and removing irrelevant features\n",
    "    * performing specific data cleaning and preprocessing techniques for an NLP problem\n",
    "    * addressing class imbalance in your data sample to promote fair AI\n",
    "    \n",
    "\n",
    "2. What machine learning model (or models) you would like to use that is suitable for your predictive problem and data?\n",
    "    * Are there other data preparation techniques that you will need to apply to build a balanced modeling data set for your problem and model? For example, will you need to scale your data?\n",
    " \n",
    " \n",
    "3. How will you evaluate and improve the model's performance?\n",
    "    * Are there specific evaluation metrics and methods that are appropriate for your model?\n",
    "    \n",
    "\n",
    "Think of the different techniques you have used to inspect and analyze your data in this course. These include using Pandas to apply data filters, using the Pandas `describe()` method to get insight into key statistics for each column, using the Pandas `dtypes` property to inspect the data type of each column, and using Matplotlib and Seaborn to detect outliers and visualize relationships between features and labels. If you are working on a classification problem, use techniques you have learned to determine if there is class imbalance.\n",
    "\n",
    "<b>Task</b>: Use the techniques you have learned in this course to inspect and analyze your data. You can import additional packages that you have used in this course that you will need to perform this task.\n",
    "\n",
    "<b>Note</b>: You can add code cells if needed by going to the <b>Insert</b> menu and clicking on <b>Insert Cell Below</b> in the drop-drown menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.4012916418948819\n",
      "MSE: 0.2679681253881403\n",
      "R^2: 0.43515844627293965\n"
     ]
    }
   ],
   "source": [
    "# handling missing values\n",
    "df['review_scores_rating'].fillna(df['review_scores_rating'].mean(), inplace=True)\n",
    "df['room_type'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# handeling outliers\n",
    "df['price'] = np.log1p(df['price'])  \n",
    "\n",
    "# feature engineering \n",
    "df = pd.get_dummies(df, columns=['room_type'])\n",
    "\n",
    "# feaature selection\n",
    "df.drop(columns=['host_name', 'host_location'], inplace=True)\n",
    "\n",
    "# scaling numerical features\n",
    "features = ['accommodates', 'bathrooms', 'bedrooms', 'beds', 'minimum_nights', 'availability_365', 'number_of_reviews', 'review_scores_rating']\n",
    "X = df[features]\n",
    "y = df['price']\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# model training\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# predications and evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'MAE: {mae}')\n",
    "print(f'MSE: {mse}')\n",
    "print(f'R^2: {r2}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Define Your Project Plan\n",
    "\n",
    "Now that you understand your data, in the markdown cell below, define your plan to implement the remaining phases of the machine learning life cycle (data preparation, modeling, evaluation) to solve your ML problem. Answer the following questions:\n",
    "\n",
    "* Do you have a new feature list? If so, what are the features that you chose to keep and remove after inspecting the data? \n",
    "* Explain different data preparation techniques that you will use to prepare your data for modeling.\n",
    "* What is your model (or models)?\n",
    "* Describe your plan to train your model, analyze its performance and then improve the model. That is, describe your model building, validation and selection plan to produce a model that generalizes well to new data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project plan is to keep features like accommodates, bathrooms, bedrooms, and review_scores_rating, while removing host_name and host_location. Data preparation will include filling missing values, handling outliers, one hot encoding categorical variables, and scaling numerical features. We'll use a Random Forest Regressor for modeling and the model will be trained on the training set, evaluated using MAE, MSE, and R² on the test set, and improved through tuning and cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Implement Your Project Plan\n",
    "\n",
    "<b>Task:</b> In the code cell below, import additional packages that you have used in this course that you will need to implement your project plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task:</b> Use the rest of this notebook to carry out your project plan. \n",
    "\n",
    "You will:\n",
    "\n",
    "1. Prepare your data for your model.\n",
    "2. Fit your model to the training data and evaluate your model.\n",
    "3. Improve your model's performance by performing model selection and/or feature selection techniques to find best model for your problem.\n",
    "\n",
    "Add code cells below and populate the notebook with commentary, code, analyses, results, and figures as you see fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "best MAE: 0.39247872261795774\n",
      "best MSE: 0.2512385335840639\n",
      "best R^2: 0.47042222480684825\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   2.7s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   5.3s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   2.6s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   2.6s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   7.9s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   5.2s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   7.8s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=  14.1s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   8.5s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   3.9s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   3.9s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   7.8s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=100; total time=   5.2s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=100; total time=   5.2s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=300; total time=  15.6s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=200; total time=   9.1s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=100; total time=   4.0s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=100; total time=   4.0s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=200; total time=   8.1s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   2.7s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   7.9s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   5.2s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   7.9s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   7.8s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   9.5s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   8.5s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=  12.7s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   7.8s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=100; total time=   5.1s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=200; total time=  10.3s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=100; total time=   4.5s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=100; total time=   4.4s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=200; total time=   8.9s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=100; total time=   4.0s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=100; total time=   4.0s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=200; total time=   8.0s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   2.7s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   7.9s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   5.2s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   2.6s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   2.6s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   5.2s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   4.7s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   9.4s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=  14.1s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=  12.7s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=  11.7s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=200; total time=  10.3s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=300; total time=  15.4s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=300; total time=  13.4s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=200; total time=   8.0s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   5.3s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   7.9s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   7.9s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   5.2s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   4.7s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   9.4s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=  14.2s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=  12.7s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=  11.7s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=200; total time=  10.3s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=300; total time=  15.5s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=300; total time=  13.3s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=300; total time=  10.6s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   5.3s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   2.7s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   2.6s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   5.2s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   2.6s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   5.2s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   7.8s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   9.4s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   8.5s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   3.9s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   7.8s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=  11.7s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=200; total time=  10.3s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=100; total time=   4.5s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=200; total time=   8.9s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=300; total time=  13.4s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=300; total time=  10.7s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   2.6s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   8.0s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   5.2s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   2.6s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   2.6s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   5.2s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   4.7s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   4.7s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=  14.1s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=  12.7s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   7.8s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=  11.7s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=300; total time=  15.5s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=200; total time=   9.0s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=300; total time=  13.4s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=300; total time=  10.9s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   2.6s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   5.3s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   2.6s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   5.2s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   7.9s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   7.8s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=   9.4s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   8.5s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   3.9s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   3.9s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=   7.8s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=100; total time=   5.2s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=100; total time=   5.2s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=300; total time=  15.5s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=200; total time=   9.0s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=200; total time=   8.1s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=300; total time=  11.3s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   5.3s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   8.0s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   7.9s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   7.8s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   4.7s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=  14.1s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=   8.5s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=300; total time=  12.8s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=300; total time=  11.8s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=200; total time=  10.3s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=100; total time=   4.5s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=100; total time=   4.5s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=300; total time=  13.5s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=200; total time=   8.1s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=300; total time=  11.0s\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=RandomForestRegressor(random_state=42),\n",
    "                           param_grid=param_grid,\n",
    "                           cv=5,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           n_jobs=-1,\n",
    "                           verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# the BEST parameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# predict with the bestt model\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "# evaluate the improved model\n",
    "mae_best = mean_absolute_error(y_test, y_pred_best)\n",
    "mse_best = mean_squared_error(y_test, y_pred_best)\n",
    "r2_best = r2_score(y_test, y_pred_best)\n",
    "\n",
    "print(f'best MAE: {mae_best}')\n",
    "print(f'best MSE: {mse_best}')\n",
    "print(f'best R^2: {r2_best}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model being tested with 27 different sets of parameters, and for each set, it's evaluated 5 times on different subsets of the data. In total, this results in 135 separate evaluations of the model's performance, and the best model achieved a mean absolute error of 0.392, a mean squared error (MSE) of 0.251, and an R² score of 0.470, indicating its best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
